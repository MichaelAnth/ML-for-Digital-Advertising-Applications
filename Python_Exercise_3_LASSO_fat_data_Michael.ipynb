{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_Exercise_3 - LASSO_fat_data - Michael",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichaelAnth/ML-for-Digital-Advertising-Applications/blob/master/Python_Exercise_3_LASSO_fat_data_Michael.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MQTqmgzSke2",
        "colab_type": "text"
      },
      "source": [
        "# Digital Advertising Python Exercise 3\n",
        "### **Author:** Michael Anthony\n",
        "### **Date:** November 2019\n",
        "### **Objective:** Use Lasso regression to select the most significant predictors for purchases of bobo-bars from census variables.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW0PYeIMUEkN",
        "colab_type": "text"
      },
      "source": [
        "# Document Setup and Loading Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUQeRxk2qvX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import pandas\n",
        "# sklearn.cross_validation was not importing train_test_split so the model\n",
        "# selection module is used instead\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LassoLarsCV\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv2uwDCf6pq_",
        "colab_type": "code",
        "outputId": "6bdbd147-6d39-4e35-a87f-1b2c8bfe454f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHwyhbYb4o9k",
        "colab_type": "code",
        "outputId": "c9459141-5923-4136-a275-201d5c97df36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "df=pd.read_csv('drive/My Drive/Colab Notebooks/finalmaster-ratios.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># Purchases</th>\n",
              "      <th>B01001001</th>\n",
              "      <th>B01001002</th>\n",
              "      <th>B01001003</th>\n",
              "      <th>B01001004</th>\n",
              "      <th>B01001005</th>\n",
              "      <th>B01001006</th>\n",
              "      <th>B01001007</th>\n",
              "      <th>B01001008</th>\n",
              "      <th>B01001009</th>\n",
              "      <th>B01001010</th>\n",
              "      <th>B01001011</th>\n",
              "      <th>B01001012</th>\n",
              "      <th>B01001013</th>\n",
              "      <th>B01001014</th>\n",
              "      <th>B01001015</th>\n",
              "      <th>B01001016</th>\n",
              "      <th>B01001017</th>\n",
              "      <th>B01001018</th>\n",
              "      <th>B01001019</th>\n",
              "      <th>B01001020</th>\n",
              "      <th>B01001021</th>\n",
              "      <th>B01001022</th>\n",
              "      <th>B01001023</th>\n",
              "      <th>B01001024</th>\n",
              "      <th>B01001025</th>\n",
              "      <th>B01001026</th>\n",
              "      <th>B01001027</th>\n",
              "      <th>B01001028</th>\n",
              "      <th>B01001029</th>\n",
              "      <th>B01001030</th>\n",
              "      <th>B01001031</th>\n",
              "      <th>B01001032</th>\n",
              "      <th>B01001033</th>\n",
              "      <th>B01001034</th>\n",
              "      <th>B01001035</th>\n",
              "      <th>B01001036</th>\n",
              "      <th>B01001037</th>\n",
              "      <th>B01001038</th>\n",
              "      <th>B01001039</th>\n",
              "      <th>...</th>\n",
              "      <th>B15002013</th>\n",
              "      <th>B15002014</th>\n",
              "      <th>B15002015</th>\n",
              "      <th>B15002016</th>\n",
              "      <th>B15002017</th>\n",
              "      <th>B15002018</th>\n",
              "      <th>B15002019</th>\n",
              "      <th>B15002020</th>\n",
              "      <th>B15002021</th>\n",
              "      <th>B15002022</th>\n",
              "      <th>B15002023</th>\n",
              "      <th>B15002024</th>\n",
              "      <th>B15002025</th>\n",
              "      <th>B15002026</th>\n",
              "      <th>B15002027</th>\n",
              "      <th>B15002028</th>\n",
              "      <th>B15002029</th>\n",
              "      <th>B15002030</th>\n",
              "      <th>B15002031</th>\n",
              "      <th>B15002032</th>\n",
              "      <th>B15002033</th>\n",
              "      <th>B15002034</th>\n",
              "      <th>B15002035</th>\n",
              "      <th>B19001001</th>\n",
              "      <th>B19001002</th>\n",
              "      <th>B19001003</th>\n",
              "      <th>B19001004</th>\n",
              "      <th>B19001005</th>\n",
              "      <th>B19001006</th>\n",
              "      <th>B19001007</th>\n",
              "      <th>B19001008</th>\n",
              "      <th>B19001009</th>\n",
              "      <th>B19001010</th>\n",
              "      <th>B19001011</th>\n",
              "      <th>B19001012</th>\n",
              "      <th>B19001013</th>\n",
              "      <th>B19001014</th>\n",
              "      <th>B19001015</th>\n",
              "      <th>B19001016</th>\n",
              "      <th>B19001017</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22</td>\n",
              "      <td>206252</td>\n",
              "      <td>469.226965</td>\n",
              "      <td>31.432422</td>\n",
              "      <td>35.219052</td>\n",
              "      <td>33.628765</td>\n",
              "      <td>20.121017</td>\n",
              "      <td>12.610787</td>\n",
              "      <td>6.734480</td>\n",
              "      <td>6.225394</td>\n",
              "      <td>19.432539</td>\n",
              "      <td>28.101546</td>\n",
              "      <td>28.421543</td>\n",
              "      <td>26.390047</td>\n",
              "      <td>31.989993</td>\n",
              "      <td>31.359696</td>\n",
              "      <td>32.116052</td>\n",
              "      <td>32.213021</td>\n",
              "      <td>12.184124</td>\n",
              "      <td>18.361034</td>\n",
              "      <td>9.454454</td>\n",
              "      <td>15.175610</td>\n",
              "      <td>16.281054</td>\n",
              "      <td>11.025348</td>\n",
              "      <td>6.230243</td>\n",
              "      <td>4.518744</td>\n",
              "      <td>530.773035</td>\n",
              "      <td>31.999690</td>\n",
              "      <td>34.322091</td>\n",
              "      <td>32.649380</td>\n",
              "      <td>20.101623</td>\n",
              "      <td>12.513818</td>\n",
              "      <td>8.072649</td>\n",
              "      <td>6.021760</td>\n",
              "      <td>22.923414</td>\n",
              "      <td>31.335454</td>\n",
              "      <td>31.558482</td>\n",
              "      <td>31.063941</td>\n",
              "      <td>36.082074</td>\n",
              "      <td>34.845723</td>\n",
              "      <td>...</td>\n",
              "      <td>64.610300</td>\n",
              "      <td>31.449746</td>\n",
              "      <td>58.735313</td>\n",
              "      <td>20.071053</td>\n",
              "      <td>6.726751</td>\n",
              "      <td>5.882267</td>\n",
              "      <td>543.803963</td>\n",
              "      <td>6.974272</td>\n",
              "      <td>2.504332</td>\n",
              "      <td>5.904107</td>\n",
              "      <td>11.917415</td>\n",
              "      <td>10.767170</td>\n",
              "      <td>18.141844</td>\n",
              "      <td>19.779852</td>\n",
              "      <td>10.956451</td>\n",
              "      <td>181.418442</td>\n",
              "      <td>26.717724</td>\n",
              "      <td>85.271036</td>\n",
              "      <td>54.243532</td>\n",
              "      <td>72.647457</td>\n",
              "      <td>30.816383</td>\n",
              "      <td>2.831933</td>\n",
              "      <td>2.912014</td>\n",
              "      <td>1000</td>\n",
              "      <td>105.667996</td>\n",
              "      <td>82.298375</td>\n",
              "      <td>68.141163</td>\n",
              "      <td>67.336195</td>\n",
              "      <td>63.566902</td>\n",
              "      <td>59.439845</td>\n",
              "      <td>49.409690</td>\n",
              "      <td>53.306757</td>\n",
              "      <td>42.318307</td>\n",
              "      <td>83.167229</td>\n",
              "      <td>89.249208</td>\n",
              "      <td>102.141470</td>\n",
              "      <td>52.872330</td>\n",
              "      <td>36.440765</td>\n",
              "      <td>23.446284</td>\n",
              "      <td>21.197485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>61399</td>\n",
              "      <td>486.538869</td>\n",
              "      <td>22.899396</td>\n",
              "      <td>21.531295</td>\n",
              "      <td>27.036271</td>\n",
              "      <td>16.808091</td>\n",
              "      <td>28.355511</td>\n",
              "      <td>18.192479</td>\n",
              "      <td>13.534422</td>\n",
              "      <td>21.466148</td>\n",
              "      <td>24.886399</td>\n",
              "      <td>23.534585</td>\n",
              "      <td>21.319565</td>\n",
              "      <td>27.101419</td>\n",
              "      <td>30.961416</td>\n",
              "      <td>37.117868</td>\n",
              "      <td>36.466392</td>\n",
              "      <td>12.557208</td>\n",
              "      <td>20.554081</td>\n",
              "      <td>12.182609</td>\n",
              "      <td>15.651721</td>\n",
              "      <td>20.668089</td>\n",
              "      <td>15.961172</td>\n",
              "      <td>10.423623</td>\n",
              "      <td>7.329110</td>\n",
              "      <td>513.461131</td>\n",
              "      <td>18.974250</td>\n",
              "      <td>23.404290</td>\n",
              "      <td>23.892897</td>\n",
              "      <td>17.036108</td>\n",
              "      <td>35.310021</td>\n",
              "      <td>18.534504</td>\n",
              "      <td>17.101256</td>\n",
              "      <td>22.785387</td>\n",
              "      <td>22.150198</td>\n",
              "      <td>22.622518</td>\n",
              "      <td>21.303279</td>\n",
              "      <td>26.971123</td>\n",
              "      <td>32.329517</td>\n",
              "      <td>...</td>\n",
              "      <td>56.929829</td>\n",
              "      <td>46.381727</td>\n",
              "      <td>65.707446</td>\n",
              "      <td>35.509451</td>\n",
              "      <td>16.782205</td>\n",
              "      <td>9.201536</td>\n",
              "      <td>515.086529</td>\n",
              "      <td>3.017306</td>\n",
              "      <td>1.047329</td>\n",
              "      <td>1.371503</td>\n",
              "      <td>6.358785</td>\n",
              "      <td>4.937410</td>\n",
              "      <td>8.303825</td>\n",
              "      <td>9.700264</td>\n",
              "      <td>7.555733</td>\n",
              "      <td>174.155902</td>\n",
              "      <td>25.834123</td>\n",
              "      <td>60.146626</td>\n",
              "      <td>62.440776</td>\n",
              "      <td>76.604658</td>\n",
              "      <td>55.383771</td>\n",
              "      <td>8.977108</td>\n",
              "      <td>9.251409</td>\n",
              "      <td>1000</td>\n",
              "      <td>71.289558</td>\n",
              "      <td>59.062447</td>\n",
              "      <td>54.704688</td>\n",
              "      <td>60.966323</td>\n",
              "      <td>53.012354</td>\n",
              "      <td>60.881706</td>\n",
              "      <td>59.231680</td>\n",
              "      <td>50.093078</td>\n",
              "      <td>40.700626</td>\n",
              "      <td>92.612963</td>\n",
              "      <td>117.363344</td>\n",
              "      <td>113.344051</td>\n",
              "      <td>75.774243</td>\n",
              "      <td>33.000508</td>\n",
              "      <td>33.169741</td>\n",
              "      <td>24.792689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>73170</td>\n",
              "      <td>489.859232</td>\n",
              "      <td>28.905289</td>\n",
              "      <td>36.271696</td>\n",
              "      <td>28.235616</td>\n",
              "      <td>21.566216</td>\n",
              "      <td>12.218122</td>\n",
              "      <td>7.243406</td>\n",
              "      <td>7.380074</td>\n",
              "      <td>16.933169</td>\n",
              "      <td>24.914582</td>\n",
              "      <td>26.896269</td>\n",
              "      <td>31.802651</td>\n",
              "      <td>30.531639</td>\n",
              "      <td>36.258029</td>\n",
              "      <td>35.998360</td>\n",
              "      <td>33.429001</td>\n",
              "      <td>13.625803</td>\n",
              "      <td>19.406861</td>\n",
              "      <td>12.245456</td>\n",
              "      <td>14.664480</td>\n",
              "      <td>21.169878</td>\n",
              "      <td>15.293153</td>\n",
              "      <td>8.610086</td>\n",
              "      <td>6.259396</td>\n",
              "      <td>510.140768</td>\n",
              "      <td>26.171928</td>\n",
              "      <td>30.681973</td>\n",
              "      <td>31.925653</td>\n",
              "      <td>19.789531</td>\n",
              "      <td>10.072434</td>\n",
              "      <td>5.056717</td>\n",
              "      <td>6.218396</td>\n",
              "      <td>15.757824</td>\n",
              "      <td>24.449911</td>\n",
              "      <td>26.595599</td>\n",
              "      <td>27.210605</td>\n",
              "      <td>37.556376</td>\n",
              "      <td>37.050704</td>\n",
              "      <td>...</td>\n",
              "      <td>54.602613</td>\n",
              "      <td>40.613027</td>\n",
              "      <td>43.363788</td>\n",
              "      <td>12.280185</td>\n",
              "      <td>5.796247</td>\n",
              "      <td>3.438452</td>\n",
              "      <td>523.980745</td>\n",
              "      <td>5.422930</td>\n",
              "      <td>4.224384</td>\n",
              "      <td>11.828274</td>\n",
              "      <td>18.331860</td>\n",
              "      <td>15.089891</td>\n",
              "      <td>21.731015</td>\n",
              "      <td>18.685529</td>\n",
              "      <td>7.014441</td>\n",
              "      <td>155.241183</td>\n",
              "      <td>45.466156</td>\n",
              "      <td>71.185775</td>\n",
              "      <td>65.802142</td>\n",
              "      <td>56.272718</td>\n",
              "      <td>24.580018</td>\n",
              "      <td>1.689753</td>\n",
              "      <td>1.414677</td>\n",
              "      <td>1000</td>\n",
              "      <td>102.538696</td>\n",
              "      <td>82.960331</td>\n",
              "      <td>74.828305</td>\n",
              "      <td>79.133495</td>\n",
              "      <td>66.081252</td>\n",
              "      <td>78.245122</td>\n",
              "      <td>63.996993</td>\n",
              "      <td>47.322923</td>\n",
              "      <td>42.505211</td>\n",
              "      <td>70.420610</td>\n",
              "      <td>90.033143</td>\n",
              "      <td>98.677692</td>\n",
              "      <td>54.703249</td>\n",
              "      <td>20.125056</td>\n",
              "      <td>11.890525</td>\n",
              "      <td>16.537397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>94</td>\n",
              "      <td>251724</td>\n",
              "      <td>505.585483</td>\n",
              "      <td>32.054949</td>\n",
              "      <td>31.757004</td>\n",
              "      <td>28.102207</td>\n",
              "      <td>18.651380</td>\n",
              "      <td>12.080692</td>\n",
              "      <td>7.035483</td>\n",
              "      <td>7.686991</td>\n",
              "      <td>25.790151</td>\n",
              "      <td>42.129475</td>\n",
              "      <td>35.824951</td>\n",
              "      <td>32.058922</td>\n",
              "      <td>27.677138</td>\n",
              "      <td>33.842621</td>\n",
              "      <td>38.176733</td>\n",
              "      <td>32.722347</td>\n",
              "      <td>12.493842</td>\n",
              "      <td>16.394940</td>\n",
              "      <td>11.504664</td>\n",
              "      <td>15.914255</td>\n",
              "      <td>16.394940</td>\n",
              "      <td>13.196994</td>\n",
              "      <td>8.648361</td>\n",
              "      <td>5.446441</td>\n",
              "      <td>494.414517</td>\n",
              "      <td>33.123580</td>\n",
              "      <td>28.082344</td>\n",
              "      <td>30.171934</td>\n",
              "      <td>16.863708</td>\n",
              "      <td>9.280005</td>\n",
              "      <td>5.390825</td>\n",
              "      <td>5.609318</td>\n",
              "      <td>19.453846</td>\n",
              "      <td>35.614403</td>\n",
              "      <td>32.082757</td>\n",
              "      <td>28.809331</td>\n",
              "      <td>27.911522</td>\n",
              "      <td>32.690566</td>\n",
              "      <td>...</td>\n",
              "      <td>88.227492</td>\n",
              "      <td>44.076261</td>\n",
              "      <td>87.939148</td>\n",
              "      <td>44.404973</td>\n",
              "      <td>9.671057</td>\n",
              "      <td>7.283569</td>\n",
              "      <td>502.912274</td>\n",
              "      <td>4.509700</td>\n",
              "      <td>0.980370</td>\n",
              "      <td>3.552398</td>\n",
              "      <td>5.986021</td>\n",
              "      <td>7.398907</td>\n",
              "      <td>9.740260</td>\n",
              "      <td>10.605292</td>\n",
              "      <td>7.485410</td>\n",
              "      <td>141.242417</td>\n",
              "      <td>43.078591</td>\n",
              "      <td>84.479020</td>\n",
              "      <td>52.069156</td>\n",
              "      <td>89.836451</td>\n",
              "      <td>33.932320</td>\n",
              "      <td>4.129086</td>\n",
              "      <td>3.886877</td>\n",
              "      <td>1000</td>\n",
              "      <td>61.632139</td>\n",
              "      <td>46.526521</td>\n",
              "      <td>48.437595</td>\n",
              "      <td>54.221644</td>\n",
              "      <td>51.680322</td>\n",
              "      <td>60.066684</td>\n",
              "      <td>54.790900</td>\n",
              "      <td>48.681562</td>\n",
              "      <td>43.873381</td>\n",
              "      <td>84.717507</td>\n",
              "      <td>112.204444</td>\n",
              "      <td>127.137252</td>\n",
              "      <td>83.019904</td>\n",
              "      <td>43.731067</td>\n",
              "      <td>38.851729</td>\n",
              "      <td>40.427349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>37382</td>\n",
              "      <td>495.586111</td>\n",
              "      <td>25.413301</td>\n",
              "      <td>29.318924</td>\n",
              "      <td>26.162324</td>\n",
              "      <td>19.260607</td>\n",
              "      <td>12.893906</td>\n",
              "      <td>6.580707</td>\n",
              "      <td>7.062222</td>\n",
              "      <td>17.334546</td>\n",
              "      <td>32.930287</td>\n",
              "      <td>28.302392</td>\n",
              "      <td>28.569900</td>\n",
              "      <td>26.804344</td>\n",
              "      <td>30.549462</td>\n",
              "      <td>36.595153</td>\n",
              "      <td>42.373335</td>\n",
              "      <td>16.398267</td>\n",
              "      <td>22.871970</td>\n",
              "      <td>17.174041</td>\n",
              "      <td>15.221229</td>\n",
              "      <td>23.433738</td>\n",
              "      <td>14.391953</td>\n",
              "      <td>7.383233</td>\n",
              "      <td>8.560270</td>\n",
              "      <td>504.413889</td>\n",
              "      <td>26.563587</td>\n",
              "      <td>30.255203</td>\n",
              "      <td>24.798031</td>\n",
              "      <td>16.237761</td>\n",
              "      <td>11.101600</td>\n",
              "      <td>4.788401</td>\n",
              "      <td>5.189663</td>\n",
              "      <td>17.842812</td>\n",
              "      <td>30.014445</td>\n",
              "      <td>27.767375</td>\n",
              "      <td>30.763469</td>\n",
              "      <td>25.199294</td>\n",
              "      <td>29.613183</td>\n",
              "      <td>...</td>\n",
              "      <td>102.957039</td>\n",
              "      <td>36.711921</td>\n",
              "      <td>70.039055</td>\n",
              "      <td>33.587502</td>\n",
              "      <td>5.021387</td>\n",
              "      <td>5.244560</td>\n",
              "      <td>511.177236</td>\n",
              "      <td>2.045750</td>\n",
              "      <td>3.236005</td>\n",
              "      <td>1.525014</td>\n",
              "      <td>7.476288</td>\n",
              "      <td>4.314674</td>\n",
              "      <td>8.554956</td>\n",
              "      <td>13.204389</td>\n",
              "      <td>7.773852</td>\n",
              "      <td>130.890831</td>\n",
              "      <td>53.784638</td>\n",
              "      <td>99.311884</td>\n",
              "      <td>57.095034</td>\n",
              "      <td>76.027525</td>\n",
              "      <td>38.422912</td>\n",
              "      <td>4.351869</td>\n",
              "      <td>3.161614</td>\n",
              "      <td>1000</td>\n",
              "      <td>51.125525</td>\n",
              "      <td>58.438255</td>\n",
              "      <td>68.930434</td>\n",
              "      <td>74.717029</td>\n",
              "      <td>63.970495</td>\n",
              "      <td>59.710034</td>\n",
              "      <td>58.883378</td>\n",
              "      <td>51.761414</td>\n",
              "      <td>47.310187</td>\n",
              "      <td>81.902582</td>\n",
              "      <td>93.793717</td>\n",
              "      <td>130.103014</td>\n",
              "      <td>71.982704</td>\n",
              "      <td>36.118530</td>\n",
              "      <td>31.603714</td>\n",
              "      <td>19.648989</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 190 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   # Purchases  B01001001   B01001002  ...  B19001015  B19001016  B19001017\n",
              "0           22     206252  469.226965  ...  36.440765  23.446284  21.197485\n",
              "1            7      61399  486.538869  ...  33.000508  33.169741  24.792689\n",
              "2            3      73170  489.859232  ...  20.125056  11.890525  16.537397\n",
              "3           94     251724  505.585483  ...  43.731067  38.851729  40.427349\n",
              "4            0      37382  495.586111  ...  36.118530  31.603714  19.648989\n",
              "\n",
              "[5 rows x 190 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eYqp8uhRRTE",
        "colab_type": "text"
      },
      "source": [
        "# Interogating the Dependent Variable - Purchases\n",
        "We look at the dependant variable \"# Purchases\" to identify early challenges in predicting this variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j7P4IHUQ3ZX",
        "colab_type": "code",
        "outputId": "18c88445-e14d-4cb2-c560-607e4d3968f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "df['# Purchases'].describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     732.000000\n",
              "mean       45.493169\n",
              "std       188.405328\n",
              "min         0.000000\n",
              "25%         1.000000\n",
              "50%         4.000000\n",
              "75%        15.000000\n",
              "max      2861.000000\n",
              "Name: # Purchases, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wi81HWXROFF",
        "colab_type": "code",
        "outputId": "e646c1ee-2960-4c59-fb46-6d45628f6a96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "plt.hist(df['# Purchases'],bins=40)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([656.,  38.,   7.,   6.,   3.,   1.,   0.,   4.,   1.,   4.,   3.,\n",
              "          1.,   2.,   0.,   0.,   1.,   2.,   0.,   0.,   0.,   1.,   0.,\n",
              "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,\n",
              "          0.,   0.,   0.,   0.,   0.,   0.,   1.]),\n",
              " array([   0.   ,   71.525,  143.05 ,  214.575,  286.1  ,  357.625,\n",
              "         429.15 ,  500.675,  572.2  ,  643.725,  715.25 ,  786.775,\n",
              "         858.3  ,  929.825, 1001.35 , 1072.875, 1144.4  , 1215.925,\n",
              "        1287.45 , 1358.975, 1430.5  , 1502.025, 1573.55 , 1645.075,\n",
              "        1716.6  , 1788.125, 1859.65 , 1931.175, 2002.7  , 2074.225,\n",
              "        2145.75 , 2217.275, 2288.8  , 2360.325, 2431.85 , 2503.375,\n",
              "        2574.9  , 2646.425, 2717.95 , 2789.475, 2861.   ]),\n",
              " <a list of 40 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCQB/wdu78UejFr0zAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UylVEAU7R86i",
        "colab_type": "text"
      },
      "source": [
        "We notice from the description and histogram that this variable is extremly right skewed. Best practice would be to analyze the distribution of the predictors we plan to use and to normalize the skewed variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v60BPMjiSbqh",
        "colab_type": "text"
      },
      "source": [
        "# Creating a List of Variables and Separating Target and Predictors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae2yKapt7pNp",
        "colab_type": "code",
        "outputId": "f5077b72-8086-4faf-a027-e2a454c424da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "allvariablenames = list(df.columns.values)\n",
        "allvariablenames[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['# Purchases',\n",
              " 'B01001001',\n",
              " 'B01001002',\n",
              " 'B01001003',\n",
              " 'B01001004',\n",
              " 'B01001005',\n",
              " 'B01001006',\n",
              " 'B01001007',\n",
              " 'B01001008',\n",
              " 'B01001009']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHDFPaPP9JQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#build a list of the predictors, #\n",
        "listofallpredictors=allvariablenames[8:]\n",
        "\n",
        "#load predictors into dataframe\n",
        "predictors = df[listofallpredictors]  \n",
        "\n",
        "#load target into dataframe\n",
        "target = df['# Purchases']  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miN3ixCQThcg",
        "colab_type": "text"
      },
      "source": [
        "# Split the data into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRNgs2TCJJKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use sample code to split data for training and testing\n",
        "# split data into train and test sets, with 30% retained for test\n",
        "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF44SwBSTxrX",
        "colab_type": "text"
      },
      "source": [
        "#Fitting the Lasso Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH3uKW3mKymW",
        "colab_type": "code",
        "outputId": "a83e336c-87c1-4040-cf87-46a2214689f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model=LassoLarsCV(precompute=False,cv=10)\n",
        "model.fit(pred_train,tar_train)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.496e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.098e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.329e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.051e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=5.739e-01, previous alpha=5.739e-01, with an active set of 14 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.100e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.867e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.050e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.013e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.986e-01, previous alpha=1.960e-01, with an active set of 37 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.365e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.008e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.144e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.642e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.644e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.626e-01, previous alpha=5.354e-01, with an active set of 13 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.439e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.037e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=7.201e-01, previous alpha=7.200e-01, with an active set of 12 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=4.546e-02, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.539e-02, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 116 iterations, alpha=3.652e-02, previous alpha=3.599e-02, with an active set of 99 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.572e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.132e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.640e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.560e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.932e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.737e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.702e-01, previous alpha=1.687e-01, with an active set of 45 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.644e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.178e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.316e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.155e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.203e-01, previous alpha=3.135e-01, with an active set of 25 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.100e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 68 iterations, alpha=6.435e-02, previous alpha=6.382e-02, with an active set of 61 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.147e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.014e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.010e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.693e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.099e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.948e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.137e-01, previous alpha=3.116e-01, with an active set of 27 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.644e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.822e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=2.062e-01, previous alpha=1.984e-01, with an active set of 34 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.372e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.825e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.732e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.285e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.186e-01, previous alpha=5.185e-01, with an active set of 15 regressors.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LassoLarsCV(copy_X=True, cv=10, eps=2.220446049250313e-16, fit_intercept=True,\n",
              "            max_iter=500, max_n_alphas=1000, n_jobs=None, normalize=True,\n",
              "            positive=False, precompute=False, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NSM0bgp_p5B",
        "colab_type": "text"
      },
      "source": [
        "# Question 1\n",
        "Explain each line of Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlbu_FpnL6bj",
        "colab_type": "code",
        "outputId": "93dd8758-b994-4aa9-9124-5a304ef828fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "source": [
        "#build coefficent chart\n",
        "#### QUESTION 1: Explain each line of Code ####\n",
        "\n",
        "# In the next line, we build a data frame containing all of the predictor names. \n",
        "predictors_model=pd.DataFrame(listofallpredictors)\n",
        "\n",
        "# Here, we change the name of the column in our new data frame to label.\n",
        "predictors_model.columns = ['label']\n",
        "\n",
        "# We create a second coulmn on the data frame called coefficient, we will now \n",
        "# have a place to store a label and a coefficeient for each predictor. \n",
        "predictors_model['coeff'] = model.coef_\n",
        "\n",
        "# iterate through the predictor model table and print the rows where the \n",
        "# coefficient is greater  than zero (It would be best to print out the rows where\n",
        "# the absolute value of the coeff value is greater than zero so we could capture \n",
        "# predictors with an inverse relationship to purchases as well..)\n",
        "for index, row in predictors_model.iterrows():\n",
        "     if row['coeff'] > 0:\n",
        "         print(row.values)\n",
        "mask=predictors_model['coeff']>0\n",
        "signif_predictors=predictors_model[mask]\n",
        "signif_predictors"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['B01001014' 0.8558761066941788]\n",
            "['B01001036' 2.5053482381631653]\n",
            "['B01001037' 0.8892493223320962]\n",
            "['B01001038' 1.5316387928880384]\n",
            "['B02001005' 0.41252295298457853]\n",
            "['B13014026' 0.48004105312075906]\n",
            "['B13014027' 0.6978957445987839]\n",
            "['B13016001' 875149895.329212]\n",
            "['B19001017' 1.4834348068681533]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>coeff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>B01001014</td>\n",
              "      <td>8.558761e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>B01001036</td>\n",
              "      <td>2.505348e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>B01001037</td>\n",
              "      <td>8.892493e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>B01001038</td>\n",
              "      <td>1.531639e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>B02001005</td>\n",
              "      <td>4.125230e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>B13014026</td>\n",
              "      <td>4.800411e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>B13014027</td>\n",
              "      <td>6.978957e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>B13016001</td>\n",
              "      <td>8.751499e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>B19001017</td>\n",
              "      <td>1.483435e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         label         coeff\n",
              "6    B01001014  8.558761e-01\n",
              "28   B01001036  2.505348e+00\n",
              "29   B01001037  8.892493e-01\n",
              "30   B01001038  1.531639e+00\n",
              "46   B02001005  4.125230e-01\n",
              "96   B13014026  4.800411e-01\n",
              "97   B13014027  6.978957e-01\n",
              "113  B13016001  8.751499e+08\n",
              "181  B19001017  1.483435e+00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGX8GosPVB1P",
        "colab_type": "text"
      },
      "source": [
        "# Find websites that explain each census variable and append these websites to the explanitory variable table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "betMCBj4N6C8",
        "colab_type": "code",
        "outputId": "86f70e94-8edc-4a9d-9529-5d4eaceb9ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# load packages to perform google search\n",
        "!pip install beautifulsoup4\n",
        "!pip install google\n",
        "from googlesearch import search "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (2.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google) (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2hRsrboQGSL",
        "colab_type": "code",
        "outputId": "71cf2272-ff73-4a80-ad81-e45e51e1dc75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "source": [
        "#we now write a code to search for \"census \" followed by the census variable name for each of the variables we found to be significant\n",
        "census_variable_websites=[]\n",
        "for i in  range(len(signif_predictors.label)):  \n",
        "    query = 'census %s' %signif_predictors.label[i:i+1].to_string(index=False)\n",
        "    for j in search(query, tld=\"co.in\", num=10, stop=1, pause=.2):\n",
        "        print(j) \n",
        "        census_variable_websites.append(j)\n",
        "# next we append these reference sites to our significant predictors table so we keep them connected to their respective variable\n",
        "signif_predictors['census_variable_website_reference']=census_variable_websites\n",
        "signif_predictors"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.socialexplorer.com/data/ACS2015_5yr/metadata/?ds=ACS15_5yr&var=B01001014\n",
            "https://www.socialexplorer.com/data/ACS2010/metadata/?ds=ACS10&var=B01001036\n",
            "https://www.socialexplorer.com/data/ACS2014/metadata/?ds=ACS14&var=B01001037\n",
            "https://www.socialexplorer.com/data/ACS2010/metadata/?ds=ACS10&var=B01001038\n",
            "https://www.socialexplorer.com/data/ACS2012_5yr/metadata/?ds=ACS12_5yr&var=B02001005\n",
            "https://www2.census.gov/programs-surveys/acs/summary_file/2009/documentation/3_year/user_tools/ACS2009_3-Year_TableShells.xls?\n",
            "https://www2.census.gov/programs-surveys/acs/summary_file/2009/documentation/3_year/user_tools/ACS2009_3-Year_TableShells.xls?\n",
            "https://www2.census.gov/acs2010_1yr/summaryfile/ACS2010_1-Year_TableShells.xls\n",
            "https://www.socialexplorer.com/data/ACS2009_5yr/metadata/?ds=ACS09_5yr&var=B19001017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>coeff</th>\n",
              "      <th>census_variable_website_reference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>B01001014</td>\n",
              "      <td>8.558761e-01</td>\n",
              "      <td>https://www.socialexplorer.com/data/ACS2015_5y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>B01001036</td>\n",
              "      <td>2.505348e+00</td>\n",
              "      <td>https://www.socialexplorer.com/data/ACS2010/me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>B01001037</td>\n",
              "      <td>8.892493e-01</td>\n",
              "      <td>https://www.socialexplorer.com/data/ACS2014/me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>B01001038</td>\n",
              "      <td>1.531639e+00</td>\n",
              "      <td>https://www.socialexplorer.com/data/ACS2010/me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>B02001005</td>\n",
              "      <td>4.125230e-01</td>\n",
              "      <td>https://www.socialexplorer.com/data/ACS2012_5y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>B13014026</td>\n",
              "      <td>4.800411e-01</td>\n",
              "      <td>https://www2.census.gov/programs-surveys/acs/s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>B13014027</td>\n",
              "      <td>6.978957e-01</td>\n",
              "      <td>https://www2.census.gov/programs-surveys/acs/s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>B13016001</td>\n",
              "      <td>8.751499e+08</td>\n",
              "      <td>https://www2.census.gov/acs2010_1yr/summaryfil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>B19001017</td>\n",
              "      <td>1.483435e+00</td>\n",
              "      <td>https://www.socialexplorer.com/data/ACS2009_5y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         label         coeff                  census_variable_website_reference\n",
              "6    B01001014  8.558761e-01  https://www.socialexplorer.com/data/ACS2015_5y...\n",
              "28   B01001036  2.505348e+00  https://www.socialexplorer.com/data/ACS2010/me...\n",
              "29   B01001037  8.892493e-01  https://www.socialexplorer.com/data/ACS2014/me...\n",
              "30   B01001038  1.531639e+00  https://www.socialexplorer.com/data/ACS2010/me...\n",
              "46   B02001005  4.125230e-01  https://www.socialexplorer.com/data/ACS2012_5y...\n",
              "96   B13014026  4.800411e-01  https://www2.census.gov/programs-surveys/acs/s...\n",
              "97   B13014027  6.978957e-01  https://www2.census.gov/programs-surveys/acs/s...\n",
              "113  B13016001  8.751499e+08  https://www2.census.gov/acs2010_1yr/summaryfil...\n",
              "181  B19001017  1.483435e+00  https://www.socialexplorer.com/data/ACS2009_5y..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrnUtuPK_ZeU",
        "colab_type": "text"
      },
      "source": [
        "# Question 2\n",
        "## *Explain what each variable means in terms of the census variable and the coefficients impact on profit.*\n",
        "\n",
        "\n",
        "##['B01001014' 0.8558761066941788]\n",
        "The first label corresponds to the percent of the population that is male and age 40 and 44. This model predicts a 0.86 unit increase in the number of purchases in an area for each percentage point increase in this demographic.\n",
        "##['B01001036' 2.5053482381631653]\n",
        "The next variable corresponds to the percent of the population that is female and between 30 and 36 years old - for a percentage point increase in this demographic variable the model predicts a 2.5 unit increase in purchases in that area.\n",
        "##['B01001037' 0.8892493223320962]\n",
        "This variable corresponds to the percent of the population that is female and between 35 and 39 years old - for a percentage point increase in this demographic variable the model predicts a 0.88 unit increase in purchases in that area.\n",
        "##['B01001038' 1.5316387928880384]\n",
        "This label corresponds to the percent of the population that is female and age 40 and 44. This model predicts a 1.53 unit increase in the number of purchases in an area for each percentage point increase in this demographic. This is higher than the 0.86 coefficient for our male 40 to 44 variable, indicating that a higher proportion of middle-aged females is likely to drive purchases more substantially than middle-aged males.\n",
        "##['B02001005' 0.41252295298457853]\n",
        "This variable corresponds to the percentage of the population that is Asian; for each percent increase in the proportion of the population that is Asian, the model predicts a 0.4 unit increase in purchases.\n",
        "##['B13014026' 0.48004105312075906]\n",
        "This variable corresponds to the proportion of the population composed of unmarried women who did not have a child in the last year and have a bachelors degree. For each percent increase in this variable the model predicts a 0.48 unit increase in sales.\n",
        "##['B13014027' 0.6978957445987839]\n",
        "This variable corresponds to the proportion of the population composed of unmarried women who did not have a child in the last year and have a graduate or professional degree. For each percent increase in this variable the model predicts a 0.7 unit increase in sales. By looking at this and the previous variable, we see that educated women are a likely purchasing base of this product - bobo bars.\n",
        "##['B13016001' 875149895.329212]\n",
        "This variable corresponds to the percent of the population that is female and 15 to 50 who had a baby in the last 12 months. For each percent increase in this variable the model predicts a 875,149,895 unit increase in sales. This coefficient is extremely high; this indicates that new mothers are strong purchasers of bobobars, and that the overall proportion of the population in this group is very small and hence a full percentage point increase in this variable is highly unlikely.\n",
        "##['B19001017' 1.4834348068681533]\n",
        "This variable corresponds to the percent of households where the household income exceeds $200,000 per year - for a percentage point increase in this demographic variable the model predicts a 1.5 unit increase in purchases in that area.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnn0Q82VU6-g",
        "colab_type": "text"
      },
      "source": [
        "# Question 3\n",
        "*If I had to report only two census variables to my boss that most steeply predicted sales, what would those be?*\n",
        "\n",
        "If I could only report two predictor variables I would report the census variable 'B13016001' and 'B01001036' which have the two largest coefficients. These two variables correspond to the proportion of the population that is female and had a baby within the last year and is between 15 and 50, and the proportion of the population that is female and 30 to 36. The percentage of the population made up of new moms is the predictor with the highest coefficient in this Lasso Regression meaning that a percentage point increase in this population is likely to drive purchases of bobobars more substantially than an increase of 1 percent in any other demographic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh84fqt4YLwm",
        "colab_type": "text"
      },
      "source": [
        "#  Question 4 - Mean Squared Error\n",
        "Here, the test data has a Mean Squared Error (MSE) nearly twice that of the training data (approximately 42,000 vs 22,000 respectively). Mean Squared Error is the average difference between the predicted and actual value. Effectively, this value helps us determine how far off we can expect our predictions to be from reality. By observing that the MSE of the training set is half that of the test set we conclude that the model predicts values closer to the actual value for the training set than for the test set - this is logical as the regression was created from the training data. by taking the square root of the MSE we can estimate the average number of units off from the actual value our prediction will be. sqrt(41550) is 203; hence, we can assume that our prediction for the number of purchases will diverge from the actual number of purchases by 203 units. Here, we are regressing on an un-normalized right-skewed variable and hence, this MSE value may be indicating the average magnitude by which we systematically overestimate purchases using this linear model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3iEWypBYZCh",
        "colab_type": "code",
        "outputId": "0f3494d1-7700-4d6e-ae99-cabd83334ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "# mean squared error \n",
        "from sklearn.metrics import mean_squared_error\n",
        "train_error = mean_squared_error(tar_train, model.predict(pred_train))\n",
        "print ('training data MSE')\n",
        "print(train_error)\n",
        "\n",
        "test_error = mean_squared_error(tar_test, model.predict(pred_test))\n",
        "print ('test data MSE')\n",
        "print(test_error)\n",
        "print(math.sqrt(41549.54803776253))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data MSE\n",
            "22025.491066757\n",
            "test data MSE\n",
            "41549.54803776253\n",
            "203.83706247334544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_4eE2ssYrtI",
        "colab_type": "text"
      },
      "source": [
        "# R-Squared Comparison\n",
        "The r squared value for the training data is .24 and .18 for the test data. This means that in the case of the training data, our multiple regression model explains 24% of the variance present. In the case of the test data, our model only explains 18% of the variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKzuYw_TYxAN",
        "colab_type": "code",
        "outputId": "f6c92889-7751-4f4b-c858-d1f550be77ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "#r squared\n",
        "rsquared_train=model.score(pred_train,tar_train)\n",
        "print ('training data R-square')\n",
        "print(rsquared_train)\n",
        "rsquared_test=model.score(pred_test,tar_test)\n",
        "print ('test data R-square')\n",
        "print(rsquared_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data R-square\n",
            "0.2400221219784492\n",
            "test data R-square\n",
            "0.1758628512005107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK0UR_8jc_UV",
        "colab_type": "text"
      },
      "source": [
        "# Question 5 - Census data as a predictor\n",
        "In the analysis of the r-squared values, we see that this multiple regression model only explains a small portion of the overall variance in the test data (18%). This implies that there are omitted variables that could not be captured in this study that determine the purchases by location. This could be things like promotions, the number of stores offering the product in an area or other factors. Demographic variables help us explain some of the trends in bobo-bar purchases, and hence could help guide managerial decision making such as marketing or ad spend. However, demographic variables do not capture a substantial enough portion of the variation in purchases to proceed with decision making without further study."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0KYtv0IeEx3",
        "colab_type": "text"
      },
      "source": [
        "# Question 6 - Baseline Sales\n",
        "With a y-intercept of 22, this model predicts a baseline sale of 22 bars per location. This means that upon entering a new location we could predict a sale of 22 bars. This number is then increased proportionally based on the demographic factors outlined in the multiple regression model and their corresponding coefficients. This number is a baseline and only increases based on demographic variables as we only retained predictors with positive coefficients.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcULpEEueF2P",
        "colab_type": "code",
        "outputId": "a35f8197-5217-41fd-9fa4-28876b634968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"y interecept:\")\n",
        "print(model.intercept_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y interecept:\n",
            "22.19738813257551\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}